---
title: "Retour sur la conférence &quot;Conflictualités sociales et environnementales autour de l’automation intelligente.&quot;"
date: "2023-08-03T12:13:30+02:00"
url: "/actualites/2023-08-03-retour-sur-la-conference-conflictualites-sociales-et-environnementales-autour-de-lautomation-intelligente/"
slug: "retour-sur-la-conference-conflictualites-sociales-et-environnementales-autour-de-lautomation-intelligente"

full_width: false
design:
  full_width: false
  toc:
    present: true
    offcanvas: false

authors:
  - "magali-angles"
categories:
  - "carnet-de-recherche"
translation_key: communication-website-post-3ff0dd38-8f0a-4a83-a042-b9762d279f21


meta_description: >
  
description: >
  

summary: >
  Forum de sociologie 2023, Université de Genève, 19 avril 2023
description_short: >
  Forum de sociologie 2023, Université de Genève, 19 avril 2023

contents:
  - kind: block
    template: chapter
    title: >
      
    position: 1
    data:
      layout: accent_background
      text: >-
        <p>Cette conférence a lieu dans le cadre du forum de sociologie 2023 organisé au sein de l'Université de Genève. Elle porte sur le malaise dans l'éthique de l'IA. La présentation est animée par Antonio Casili, professeur de sociologie à Telecom Paris, Institut Polytechnique de Paris, et chercheur à l'Institut Interdisciplinaire de l'Innovation. Il y aura une discussion avec Claire BALLEYS (Université de Genève) et Nicolas NOVA (HEAD HES-SO Genève).</p>


      notes: >-
        




      alt: >-
        


      credit: >-
        




  - kind: block
    template: organization_chart
    title: >
      
    position: 2
    data:
      description: >-
        


      with_link: true

      with_photo: true

      persons:
        - slug: "antonio-casilli"
          role: >-
            


        - slug: "nicolas-nova"
          role: >-
            


        - slug: "claire-balleys"
          role: >-
            




  - kind: block
    template: chapter
    title: >
      
    position: 3
    data:
      layout: no_background
      text: >-
        <p>L'intelligence artificielle existe depuis 70 ans. Le domaine de recherche est plus récent, datant de 2016/2017. Depuis lors, la production d'IA éthique a été immense, voire excessive.</p><p>Le mythe de l'IA est celui d'une intelligence artificielle dépassant l'intelligence humaine dans tous les domaines. Certains domaines critiques comprennent l'armement, les voitures autonomes, les contextes judiciaires et policiers.</p><p>  </p><p>Antonio Casilli pose la question suivante : qui est autorisé à parler et à définir ce qu'est une IA éthique ?</p>


      notes: >-
        




      alt: >-
        


      credit: >-
        




  - kind: heading
    title: >-
      Lorsqu'on s'inquiète des conséquences éthiques
    position: 1
    level: 2
  - kind: block
    template: chapter
    title: >
      
    position: 1
    data:
      layout: alt_background
      text: >-
        <p>Dernièrement, une lettre ouverte signée par des milliers de scientifiques et d'industriels, tels qu'Elon Musk, a demandé l'arrêt du développement de nouvelles IA pendant six mois. Ces personnes sont des défenseurs de l'éthique de l'IA, mais ce sont les mêmes qui ont signé une lettre en 2015 marquant le début de l'IA.</p><p>Depuis 2015, 167 chartes éthiques pour l'IA ont été créées, fournissant des orientations, des normes et des meilleures pratiques pour l'application de l'IA. Cependant, cette application est encore à débattre. Parmi les signataires se trouvent des agences étatiques, des institutions internationales, des scientifiques, des groupes d'intérêts et des producteurs, illustrant l'investissement politique dans ces orientations.</p><p>En 2019, la revue Nature a identifié cinq thèmes relatifs à l'éthique de l'IA :</p><p>1. Transparence : Qu'est-ce qui se cache derrière les chatbots GPT ? L'IA accusée d'être une blackbox. Incitations à la mise en place d'une IA transparente.</p><p>2. Justice et équité : les IA peuvent favoriser ou défavoriser certaines populations à travers des biais algorithmiques </p><p>3. Non-malfaisance : ces IA ne devraient pas causer des dommages permanents et graves à des populations, par exemple, dans des contextes de guerre et d'armement. La question se pose de savoir s'il est juste d'utiliser des systèmes d'IA pour cibler des missiles ou des drones.</p><p>4. Responsabilité : Moral cralw machine : résoudre des dilemmes éthiques. Ces situations sont rarement extrêmes et plutôt de nature juridique.</p><p>5. Protection de la confidentialité : vie privée des utilisateurs. Un incident de vie privée a eu lieu il y a un mois et demi sur ChatGPT : l'historique de chat d'autres personnes a été exposée, révélant une faille de protection des données. Des enquêtes ont été lancées contre ChatGPT pour qu’il se conforme à la législation européenne.</p><p>Antonio Casilli remarque que bien que ces grandes valeurs divergent, il y a un aspect important : ces valeurs se concentrent sur l'utilisation. Cependant, certaines valeurs importantes telles que la soutenabilité, la confiance, l'autonomie, la solidarité et la dignité sont absentes.<br></p>


      notes: >-
        




      alt: >-
        


      credit: >-
        





  - kind: heading
    title: >-
      Liens troubles avec l’industrie
    position: 2
    level: 2
  - kind: block
    template: chapter
    title: >
      
    position: 1
    data:
      layout: alt_background
      text: >-
        <p>Antonio Casilli nous met en garde : le même discours est tenu depuis 2015, il faut trouver une éthique qui ne freine pas les producteurs d'IA.</p><p>La convergence, l'alliance et la concertation entre les personnes qui font de la recherche sur l'éthique et les producteurs de technologie sont de plus en plus floues. Les auteurs affiliés aux GAFAM sont passés de 43 à 79 %, ce qui signifie que la majorité des auteurs de papiers présentés dans les conférences proviennent de ces entreprises.</p><p>  </p><p>Il soulève également le problème suivant : l'éthique n'est-elle qu'une préoccupation du Nord ? Les pays du Sud sont sous-représentés dans ce domaine, ce qui peut nuire au pluralisme et à l'ouverture culturelle. Il constate que la Chine est complètement absente : il n'y a pas de charte éthique chinoise traduite.</p>


      notes: >-
        




      alt: >-
        


      credit: >-
        





  - kind: heading
    title: >-
      On se concentre sur la phase d’usage (Ethics by design), mais il faut se concentrer aussi sur la phase de production
    position: 3
    level: 2
  - kind: block
    template: chapter
    title: >
      
    position: 1
    data:
      layout: alt_background
      text: >-
        <p>Antonio Casilli montre qu'il existe trois contributions humaines dans le travail du clic pour la production de l'IA : la phase de préparation, de vérification et d'imitation.</p><ol> <li>Préparation de l'IA : il s'agit de la création de données pour l'entraînement. Par exemple, en 2017, certaines personnes devaient entourer des tomates pour entraîner l’IA. Les personnes faisaient cela sans comprendre pourquoi cela était nécessaire. On peut parler d’aliénation détachée au sens de Marx.</li> <li>Vérification de l'IA : un processus essentiel pour garantir sa qualité et contribuer. L'IA est déjà présente sur le marché. Il cite, par exemple, des personnes qui peuvent être chargées de vérifier les enregistrements d'un assistant vocal. Cela soulève des problèmes considérables en matière de vie privée. L'Union Européenne est un important producteur d'IA et a recours à des plateformes chinoises pour effectuer des vérifications. Ces plateformes se tournent ensuite vers des plateformes japonaises, puis espagnoles, avant de renvoyer les données aux États-Unis.</li> <li><p>Imitation de l'IA : il cite comme exemple les caméras intelligentes dans les magasins comme Carrefour ou Monoprix, censées détecter les personnes susceptibles de voler. Elles détectent un geste et savent si c'est anodin ou non. Un message automatique est envoyé au vigile ou au caissier. Environ 127 personnes à Madagascar effectuent ce travail de caméra intelligente : surveillance à distance en flux tendu. Il n'y a pas d'algorithmes. Antonio Casilli confirme que c'est une pratique répandue, mais il est nécessaire de tester pour que l'IA fonctionne correctement. Elle peut donner un coup de pouce, mais nous avons encore besoin d'intervention humaine. Malgré cela, il n'y a pas eu d'avancées significatives au cours des cinq dernières années.</p></li> </ol>


      notes: >-
        




      alt: >-
        


      credit: >-
        






---

